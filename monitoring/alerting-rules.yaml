# Alerting Rules for Production Monitoring
# PagerDuty / Opsgenie / Slack integration

groups:
  - name: dlq_alerts
    interval: 1m
    rules:
      - alert: HighDLQPendingCount
        expr: |
          SELECT COUNT(*) as pending_count 
          FROM webhook_dlq 
          WHERE status = 'pending'
        condition: pending_count > 100
        severity: warning
        annotations:
          summary: "High number of pending DLQ entries"
          description: "{{ pending_count }} messages pending in DLQ. Investigate processing issues."
        
      - alert: DLQProcessingFailure
        expr: |
          SELECT MAX(processed_at) as last_processed 
          FROM dlq_processing_log
        condition: last_processed < NOW() - INTERVAL '15 minutes'
        severity: critical
        annotations:
          summary: "DLQ processor not running"
          description: "DLQ processor hasn't run in 15 minutes. Check pg_cron job status."
      
      - alert: HighDLQFailureRate
        expr: |
          SELECT COUNT(*) as failed_count 
          FROM webhook_dlq 
          WHERE status = 'failed' 
          AND created_at > NOW() - INTERVAL '1 hour'
        condition: failed_count > 10
        severity: critical
        annotations:
          summary: "High DLQ failure rate"
          description: "{{ failed_count }} messages failed permanently in the last hour. Check error patterns."

  - name: webhook_alerts
    interval: 1m
    rules:
      - alert: WebhookErrorRate
        expr: |
          SELECT 
            ROUND(100.0 * COUNT(CASE WHEN processed = false THEN 1 END) / COUNT(*), 2) as error_rate
          FROM processed_webhook_messages
          WHERE created_at > NOW() - INTERVAL '5 minutes'
        condition: error_rate > 5
        severity: warning
        annotations:
          summary: "High webhook error rate"
          description: "{{ error_rate }}% of webhooks failing in last 5 minutes."
      
      - alert: WebhookLatency
        expr: |
          SELECT 
            EXTRACT(EPOCH FROM (MAX(updated_at) - MAX(created_at))) as latency_seconds
          FROM processed_webhook_messages
          WHERE created_at > NOW() - INTERVAL '5 minutes'
        condition: latency_seconds > 5
        severity: warning
        annotations:
          summary: "High webhook processing latency"
          description: "Webhooks taking {{ latency_seconds }}s to process (>5s threshold)."

  - name: database_alerts
    interval: 5m
    rules:
      - alert: HighDatabaseBloat
        expr: |
          SELECT 
            tablename,
            ROUND(100.0 * n_dead_tup / NULLIF(n_live_tup + n_dead_tup, 0), 2) as bloat_pct
          FROM pg_stat_user_tables
          WHERE n_dead_tup > 10000
        condition: bloat_pct > 20
        severity: warning
        annotations:
          summary: "High table bloat detected"
          description: "Table {{ tablename }} has {{ bloat_pct }}% dead tuples. Consider manual VACUUM."
      
      - alert: LargeTableGrowth
        expr: |
          SELECT 
            tablename,
            pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size
          FROM pg_tables
          WHERE schemaname = 'public'
        condition: pg_total_relation_size > 10GB
        severity: info
        annotations:
          summary: "Large table detected"
          description: "Table {{ tablename }} is {{ size }}. Consider archival or partitioning."

  - name: circuit_breaker_alerts
    interval: 1m
    rules:
      - alert: CircuitBreakerOpen
        expr: |
          SELECT state, COUNT(*) as count
          FROM circuit_breaker_events
          WHERE created_at > NOW() - INTERVAL '5 minutes'
          GROUP BY state
        condition: state = 'OPEN'
        severity: critical
        annotations:
          summary: "Circuit breaker opened"
          description: "Circuit breaker in OPEN state. External service may be down."

  - name: system_health_alerts
    interval: 5m
    rules:
      - alert: HighConnectionCount
        expr: |
          SELECT COUNT(*) as connections 
          FROM pg_stat_activity 
          WHERE state != 'idle'
        condition: connections > 80
        severity: warning
        annotations:
          summary: "High database connection count"
          description: "{{ connections }} active database connections. Check for connection leaks."
      
      - alert: LongRunningQueries
        expr: |
          SELECT 
            pid,
            EXTRACT(EPOCH FROM (NOW() - query_start)) as duration_seconds,
            LEFT(query, 100) as query_preview
          FROM pg_stat_activity
          WHERE state = 'active'
          AND query NOT LIKE '%pg_stat_activity%'
        condition: duration_seconds > 30
        severity: warning
        annotations:
          summary: "Long-running query detected"
          description: "Query {{ pid }} running for {{ duration_seconds }}s: {{ query_preview }}"

  # =========================================================================
  # Moltbot AI Concierge Alerts
  # =========================================================================
  - name: moltbot_alerts
    interval: 5m
    rules:
      - alert: MoltbotOcrFailureRate
        expr: |
          SELECT 
            ROUND(100.0 * COUNT(CASE WHEN status = 'failed' THEN 1 END) / NULLIF(COUNT(*), 0), 2) as failure_rate
          FROM moltbot_ocr_jobs
          WHERE created_at > NOW() - INTERVAL '24 hours'
        condition: failure_rate > 10
        severity: warning
        annotations:
          summary: "High OCR failure rate"
          description: "{{ failure_rate }}% of OCR jobs failed in the last 24 hours (threshold: 10%)."
          runbook_url: "https://docs.internal/runbooks/ocr-failures"
      
      - alert: MoltbotOcrFailureCritical
        expr: |
          SELECT 
            ROUND(100.0 * COUNT(CASE WHEN status = 'failed' THEN 1 END) / NULLIF(COUNT(*), 0), 2) as failure_rate
          FROM moltbot_ocr_jobs
          WHERE created_at > NOW() - INTERVAL '24 hours'
        condition: failure_rate > 25
        severity: critical
        annotations:
          summary: "Critical OCR failure rate"
          description: "{{ failure_rate }}% of OCR jobs failed in the last 24 hours (threshold: 25%). Consider pausing OCR."
          runbook_url: "https://docs.internal/runbooks/ocr-failures"

      - alert: MoltbotAiRejectionRate
        expr: |
          SELECT 
            ROUND(100.0 * COUNT(CASE WHEN success = false THEN 1 END) / NULLIF(COUNT(*), 0), 2) as rejection_rate
          FROM moltbot_audit_events
          WHERE event_type = 'moltbot.called'
          AND created_at > NOW() - INTERVAL '24 hours'
        condition: rejection_rate > 5
        severity: warning
        annotations:
          summary: "High Moltbot output rejection rate"
          description: "{{ rejection_rate }}% of Moltbot outputs rejected by safety gate in the last 24 hours."
          runbook_url: "https://docs.internal/runbooks/moltbot-rejections"

      - alert: MoltbotVendorReplyRate
        expr: |
          SELECT 
            ROUND(100.0 * COUNT(CASE WHEN state = 'replied' THEN 1 END) / NULLIF(COUNT(*), 0), 2) as reply_rate
          FROM moltbot_vendor_outreach
          WHERE outreach_sent_at > NOW() - INTERVAL '7 days'
        condition: reply_rate < 15
        severity: warning
        annotations:
          summary: "Low vendor reply rate"
          description: "Only {{ reply_rate }}% of vendors replied in the last 7 days (threshold: 15%)."
          runbook_url: "https://docs.internal/runbooks/vendor-engagement"

      - alert: MoltbotCallFailureSpike
        expr: |
          SELECT COUNT(*) as failures
          FROM moltbot_call_attempts
          WHERE status = 'failed'
          AND initiated_at > NOW() - INTERVAL '1 hour'
        condition: failures > 5
        severity: warning
        annotations:
          summary: "Call failure spike detected"
          description: "{{ failures }} call failures in the last hour. Check provider status."
          runbook_url: "https://docs.internal/runbooks/call-failures"

      - alert: MoltbotBudgetExceededRate
        expr: |
          SELECT 
            ROUND(100.0 * COUNT(CASE WHEN event_type LIKE '%budget_exceeded%' THEN 1 END) / NULLIF(COUNT(DISTINCT request_id), 0), 2) as exceeded_rate
          FROM moltbot_audit_events
          WHERE created_at > NOW() - INTERVAL '24 hours'
        condition: exceeded_rate > 5
        severity: warning
        annotations:
          summary: "High budget exceeded rate"
          description: "{{ exceeded_rate }}% of requests hit budget limits in the last 24 hours."
          runbook_url: "https://docs.internal/runbooks/budget-exceeded"

      - alert: MoltbotTimeToShortlistSlow
        expr: |
          SELECT 
            PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY 
              EXTRACT(EPOCH FROM (updated_at - created_at))
            ) as p95_seconds
          FROM moltbot_marketplace_requests
          WHERE state IN ('shortlist_ready', 'handed_off', 'closed')
          AND created_at > NOW() - INTERVAL '6 hours'
        condition: p95_seconds > 300
        severity: warning
        annotations:
          summary: "Slow time-to-shortlist (P95)"
          description: "P95 time-to-shortlist is {{ p95_seconds }}s (threshold: 300s / 5 minutes)."
          runbook_url: "https://docs.internal/runbooks/performance"

# Notification channels configuration
notification_channels:
  - name: pagerduty_critical
    type: pagerduty
    settings:
      integration_key: "${PAGERDUTY_INTEGRATION_KEY}"
      severity_mapping:
        critical: "critical"
        warning: "warning"
        info: "info"
  
  - name: slack_alerts
    type: slack
    settings:
      webhook_url: "${SLACK_WEBHOOK_URL}"
      channel: "#alerts-production"
      username: "EasyMO Monitor"
      
  - name: email_oncall
    type: email
    settings:
      recipients:
        - oncall@easymo.com
      subject_template: "[{{ severity }}] {{ summary }}"

# Alert routing
routing:
  - match:
      severity: critical
    receiver: pagerduty_critical
    continue: true
  
  - match:
      severity: warning
    receiver: slack_alerts
    continue: true
  
  - match:
      severity: info
    receiver: email_oncall
